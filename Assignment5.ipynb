{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adde4876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff33f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test-1.txt', sep='\\t')\n",
    "train_data = pd.read_csv('train-3.txt', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee29134",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-En-01964</td>\n",
       "      <td>Gm and have a  #Tuesday!</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-En-01539</td>\n",
       "      <td>@realDonaldTrump But you have a lot of time fo...</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-En-04235</td>\n",
       "      <td>I graduated yesterday and already had 8 family...</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                              Tweet  \\\n",
       "0  2018-En-01964                           Gm and have a  #Tuesday!   \n",
       "1  2018-En-01539  @realDonaldTrump But you have a lot of time fo...   \n",
       "2  2018-En-04235  I graduated yesterday and already had 8 family...   \n",
       "\n",
       "  Affect Dimension                                    Intensity Class  \n",
       "0          valence  0: neutral or mixed emotional state can be inf...  \n",
       "1          valence  0: neutral or mixed emotional state can be inf...  \n",
       "2          valence  0: neutral or mixed emotional state can be inf...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca336c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Affect Dimension</th>\n",
       "      <th>Intensity Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-En-30153</td>\n",
       "      <td>@liamch88 yeah! :) playing well</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-En-40929</td>\n",
       "      <td>At least I don't have a guy trying to discoura...</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-En-22012</td>\n",
       "      <td>UPLIFT: If you're still discouraged it means y...</td>\n",
       "      <td>valence</td>\n",
       "      <td>0: neutral or mixed emotional state can be inf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                              Tweet  \\\n",
       "0  2017-En-30153                   @liamch88 yeah! :) playing well    \n",
       "1  2017-En-40929  At least I don't have a guy trying to discoura...   \n",
       "2  2017-En-22012  UPLIFT: If you're still discouraged it means y...   \n",
       "\n",
       "  Affect Dimension                                    Intensity Class  \n",
       "0          valence  0: neutral or mixed emotional state can be inf...  \n",
       "1          valence  0: neutral or mixed emotional state can be inf...  \n",
       "2          valence  0: neutral or mixed emotional state can be inf...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fd43b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'Tweet', 'Affect Dimension', 'Intensity Class'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebb824cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mapping = {\"0: neutral or mixed emotional state can be inferred\": 'neutral',\n",
    "                    \"1: slightly positive emotional state can be inferred\": 'neutral',\n",
    "                    \"2: moderately positive emotional state can be inferred\": 'positive',\n",
    "                    \"3: very positive emotional state can be inferred\": 'positive',\n",
    "                    \"-1: slightly negative emotional state can be inferred\": 'neutral',\n",
    "                    \"-2: moderately negative emotional state can be inferred\": 'negative',\n",
    "                    \"-3: very negative emotional state can be inferred\": 'negative'\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "583ecd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Intensity'] = train_data['Intensity Class'].replace(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a41494ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['Intensity'] = test_data['Intensity Class'].replace(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1c460ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(columns=['Intensity Class','ID','Affect Dimension'], inplace=True)\n",
    "test_data.drop(columns=['Intensity Class','ID','Affect Dimension'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b06529ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@liamch88 yeah! :) playing well</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>At least I don't have a guy trying to discoura...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UPLIFT: If you're still discouraged it means y...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>...at your age, the heyday in the blood is tam...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was so embarrassed when she saw us i was lik...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Really planned on making videos this week. The...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I hate having ideas but being too afraid to sh...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>At the regular cheerfulness of any emotion, he...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A pessimist sees the difficulty in every oppor...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Just because I'm hurting \\nDoesn't mean I'm hu...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet Intensity\n",
       "0                   @liamch88 yeah! :) playing well    neutral\n",
       "1  At least I don't have a guy trying to discoura...   neutral\n",
       "2  UPLIFT: If you're still discouraged it means y...   neutral\n",
       "3  ...at your age, the heyday in the blood is tam...   neutral\n",
       "4  i was so embarrassed when she saw us i was lik...  negative\n",
       "5  Really planned on making videos this week. The...  negative\n",
       "6  I hate having ideas but being too afraid to sh...  negative\n",
       "7  At the regular cheerfulness of any emotion, he...   neutral\n",
       "8  A pessimist sees the difficulty in every oppor...   neutral\n",
       "9  Just because I'm hurting \\nDoesn't mean I'm hu...  negative"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9481bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['Tweet'] = train_data['Tweet'].str.lower()\n",
    "test_data['Tweet'] = test_data['Tweet'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b84f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([train_data, test_data], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2a5c99d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/shivamsinghrawat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "df_all['tokens'] = df_all['Tweet'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f20d4433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Intensity</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@liamch88 yeah! :) playing well</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[@, liamch88, yeah, !, :, ), playing, well]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>at least i don't have a guy trying to discoura...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[at, least, i, do, n't, have, a, guy, trying, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>uplift: if you're still discouraged it means y...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[uplift, :, if, you, 're, still, discouraged, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>...at your age, the heyday in the blood is tam...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[..., at, your, age, ,, the, heyday, in, the, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was so embarrassed when she saw us i was lik...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, was, so, embarrassed, when, she, saw, us, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>really planned on making videos this week. the...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[really, planned, on, making, videos, this, we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>i hate having ideas but being too afraid to sh...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[i, hate, having, ideas, but, being, too, afra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>at the regular cheerfulness of any emotion, he...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[at, the, regular, cheerfulness, of, any, emot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a pessimist sees the difficulty in every oppor...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[a, pessimist, sees, the, difficulty, in, ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>just because i'm hurting \\ndoesn't mean i'm hu...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[just, because, i, 'm, hurting, \\ndoes, n't, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Tweet Intensity  \\\n",
       "0                   @liamch88 yeah! :) playing well    neutral   \n",
       "1  at least i don't have a guy trying to discoura...   neutral   \n",
       "2  uplift: if you're still discouraged it means y...   neutral   \n",
       "3  ...at your age, the heyday in the blood is tam...   neutral   \n",
       "4  i was so embarrassed when she saw us i was lik...  negative   \n",
       "5  really planned on making videos this week. the...  negative   \n",
       "6  i hate having ideas but being too afraid to sh...  negative   \n",
       "7  at the regular cheerfulness of any emotion, he...   neutral   \n",
       "8  a pessimist sees the difficulty in every oppor...   neutral   \n",
       "9  just because i'm hurting \\ndoesn't mean i'm hu...  negative   \n",
       "\n",
       "                                              tokens  \n",
       "0        [@, liamch88, yeah, !, :, ), playing, well]  \n",
       "1  [at, least, i, do, n't, have, a, guy, trying, ...  \n",
       "2  [uplift, :, if, you, 're, still, discouraged, ...  \n",
       "3  [..., at, your, age, ,, the, heyday, in, the, ...  \n",
       "4  [i, was, so, embarrassed, when, she, saw, us, ...  \n",
       "5  [really, planned, on, making, videos, this, we...  \n",
       "6  [i, hate, having, ideas, but, being, too, afra...  \n",
       "7  [at, the, regular, cheerfulness, of, any, emot...  \n",
       "8  [a, pessimist, sees, the, difficulty, in, ever...  \n",
       "9  [just, because, i, 'm, hurting, \\ndoes, n't, m...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f5d2ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = df_all['tokens'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "217d7a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,    \n",
    "    window=5,          \n",
    "    min_count=5,       \n",
    "    sg=0,              \n",
    "    workers=4 \n",
    ")\n",
    "\n",
    "skipgram_model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    sg=1,              # Skip-gram (1)\n",
    "    workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7967596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Save models\n",
    "cbow_model.save(\"word2vec_cbow.model\")\n",
    "skipgram_model.save(\"word2vec_skipgram.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb8d65de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Similar Words:\n",
      "\n",
      "people:\n",
      "[('and', 0.9996721744537354), ('she', 0.9996697306632996), ('for', 0.9996540546417236), ('up', 0.9996422529220581), ('to', 0.9996315240859985), ('an', 0.9996191263198853), ('is', 0.9996159672737122), ('by', 0.9996126294136047), ('my', 0.9996110200881958), ('the', 0.9996082186698914), ('are', 0.999606192111969), ('about', 0.999605119228363), ('has', 0.9996013641357422), ('when', 0.9995970129966736), ('they', 0.9995940923690796), ('her', 0.9995924830436707), ('can', 0.999592125415802), ('a', 0.9995900392532349), ('out', 0.9995893836021423), ('being', 0.9995850324630737)]\n",
      "\n",
      "smile:\n",
      "[('this', 0.9991061091423035), ('.', 0.9990772008895874), ('-', 0.9990461468696594), ('or', 0.9990441203117371), (\"'\", 0.999039888381958), ('?', 0.9990136027336121), ('happy', 0.9990071654319763), ('on', 0.9989997744560242), ('of', 0.9989823698997498), ('for', 0.9989822506904602), ('their', 0.9989784359931946), ('is', 0.9989719986915588), ('them', 0.9989688396453857), ('your', 0.9989668130874634), ('at', 0.9989549517631531), ('im', 0.9989507794380188), ('than', 0.9989480376243591), ('life', 0.9989369511604309), ('with', 0.998933732509613), ('over', 0.9989237785339355)]\n",
      "\n",
      "amazing:\n",
      "[('a', 0.9993783831596375), ('her', 0.9993653297424316), ('by', 0.9993345737457275), ('u', 0.9993289113044739), ('in', 0.99930739402771), ('and', 0.9993035197257996), ('are', 0.9992960691452026), ('your', 0.9992899298667908), ('is', 0.9992865920066833), ('an', 0.9992822408676147), ('she', 0.99928218126297), ('their', 0.9992812275886536), ('.', 0.9992771744728088), ('this', 0.9992761015892029), ('every', 0.9992719888687134), ('who', 0.9992684125900269), ('or', 0.9992662072181702), ('about', 0.9992586970329285), ('there', 0.9992586970329285), ('day', 0.9992572665214539)]\n",
      "\n",
      "time:\n",
      "[('and', 0.9996444582939148), ('at', 0.9996197819709778), ('of', 0.9996082782745361), ('a', 0.9995948672294617), ('on', 0.9995784759521484), ('up', 0.9995651841163635), ('the', 0.999555766582489), ('her', 0.9995512366294861), ('about', 0.9995496273040771), ('an', 0.9995201230049133), ('..', 0.9995141625404358), ('in', 0.9995113611221313), ('over', 0.9995097517967224), ('is', 0.9995084404945374), ('every', 0.9995033144950867), ('.', 0.9995027780532837), ('she', 0.9995018839836121), ('this', 0.9994982481002808), ('but', 0.999498188495636), ('good', 0.9994960427284241)]\n",
      "\n",
      "Skip-gram Similar Words:\n",
      "\n",
      "people:\n",
      "[('how', 0.9944929480552673), ('someone', 0.9942940473556519), ('go', 0.9940348267555237), ('are', 0.9940067529678345), ('thing', 0.9938623309135437), ('from', 0.9937285780906677), ('by', 0.9934409260749817), ('..', 0.9933483600616455), ('an', 0.9929901957511902), ('still', 0.992962658405304), ('man', 0.9928953051567078), ('watch', 0.9928756952285767), ('where', 0.9928389191627502), ('before', 0.9926814436912537), ('see', 0.9926813244819641), ('laughing', 0.9924699664115906), ('week', 0.9923695921897888), ('there', 0.9923568367958069), ('last', 0.9923457503318787), ('what', 0.9921910762786865)]\n",
      "\n",
      "smile:\n",
      "[('laughter', 0.983650803565979), ('-', 0.9814392328262329), ('\\\\n', 0.9785009026527405), ('next', 0.9784917831420898), ('3', 0.9779739379882812), ('rejoice', 0.9776594042778015), ('something', 0.9775928854942322), ('joy', 0.9765377640724182), ('beautiful', 0.975505530834198), ('mirth', 0.9753159880638123), ('music', 0.9748309254646301), ('v', 0.9746383428573608), ('sad', 0.9745736122131348), ('trump', 0.9744638204574585), ('our', 0.9739177823066711), ('new', 0.9735767841339111), ('birthday', 0.9735410809516907), ('thenicebot', 0.9727104902267456), ('india', 0.9726120829582214), ('sister', 0.9725257158279419)]\n",
      "\n",
      "amazing:\n",
      "[('quote', 0.9964583516120911), ('ironic', 0.996363639831543), ('irony', 0.9963250756263733), ('fear', 0.9949560165405273), ('hilarious', 0.9944155216217041), ('blues', 0.9938145279884338), ('♥', 0.9930537939071655), ('lost', 0.9928357005119324), ('music', 0.9927606582641602), ('show', 0.9926370978355408), ('bright', 0.9925356507301331), ('future', 0.9923962354660034), ('sorry', 0.99199378490448), ('shocking', 0.9916968941688538), ('truth', 0.9916836619377136), ('depressing', 0.9914901852607727), ('optimism', 0.9912746548652649), ('glee', 0.9912059903144836), ('fuming', 0.9911296963691711), ('anger', 0.9910407066345215)]\n",
      "\n",
      "time:\n",
      "[('someone', 0.9926823973655701), ('from', 0.9923533797264099), ('one', 0.992117702960968), ('people', 0.9919530153274536), ('how', 0.9918299317359924), ('been', 0.9916822910308838), ('an', 0.991453230381012), ('..', 0.9909869432449341), ('there', 0.990878701210022), ('thing', 0.99080491065979), ('man', 0.9907333850860596), ('was', 0.9904313087463379), ('week', 0.9903579950332642), ('by', 0.9903567433357239), ('up', 0.9903548359870911), ('no', 0.9901925325393677), ('and', 0.9900809526443481), ('after', 0.9898101091384888), ('go', 0.9897732138633728), ('still', 0.9897469878196716)]\n"
     ]
    }
   ],
   "source": [
    "test_words = [\"people\", \"smile\", \"amazing\", \"time\"]\n",
    "\n",
    "def get_similar_words(model, word, n=20):\n",
    "    try:\n",
    "        return model.wv.most_similar(word, topn=n)\n",
    "    except KeyError:\n",
    "        return f\"{word} not in vocabulary\"\n",
    "\n",
    "# CBOW Results\n",
    "print(\"CBOW Similar Words:\")\n",
    "for word in test_words:\n",
    "    print(f\"\\n{word}:\")\n",
    "    print(get_similar_words(cbow_model, word))\n",
    "\n",
    "# Skip-gram Results\n",
    "print(\"\\nSkip-gram Similar Words:\")\n",
    "for word in test_words:\n",
    "    print(f\"\\n{word}:\")\n",
    "    print(get_similar_words(skipgram_model, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b71e5",
   "metadata": {},
   "source": [
    "## Task 2 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4d12663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "glove = KeyedVectors.load_word2vec_format(\"glove.6B.100d.100K-1.w2v.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbcd28cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "people:\n",
      "[('others', 0.8318450450897217), ('those', 0.8050370812416077), ('many', 0.7967616319656372), ('some', 0.7750677466392517), ('they', 0.7717718482017517), ('residents', 0.7647792100906372), ('them', 0.7625812888145447), ('than', 0.7592476010322571), ('all', 0.758562445640564), ('families', 0.7561567425727844), ('say', 0.755620002746582), ('there', 0.7546865344047546), ('lives', 0.7536002397537231), ('children', 0.753533124923706), ('citizens', 0.7532268166542053), ('least', 0.7485319972038269), ('more', 0.7447736263275146), ('so', 0.7420145273208618), ('have', 0.7417193651199341), ('americans', 0.7411874532699585)]\n",
      "\n",
      "smile:\n",
      "[('grin', 0.8655877709388733), ('smiles', 0.7909191846847534), ('eyes', 0.7565146088600159), ('smiling', 0.7020254135131836), ('laugh', 0.7009447813034058), ('tears', 0.6666248440742493), ('smirk', 0.6415873765945435), ('sight', 0.635871410369873), ('smiled', 0.6354667544364929), ('hug', 0.6348837614059448), ('eyed', 0.6320615410804749), ('touch', 0.6114146709442139), ('chuckle', 0.6090990900993347), ('moment', 0.6026745438575745), ('lips', 0.6019719243049622), ('scowl', 0.5972016453742981), ('bright', 0.593736469745636), ('hair', 0.592218816280365), ('me', 0.5897890329360962), ('flashing', 0.589604914188385)]\n",
      "\n",
      "amazing:\n",
      "[('incredible', 0.9194009900093079), ('fantastic', 0.8431944847106934), ('wonderful', 0.8312846422195435), ('astonishing', 0.8230013847351074), ('marvelous', 0.8176810145378113), ('awesome', 0.8062849640846252), ('unbelievable', 0.8006238341331482), ('remarkable', 0.7889993786811829), ('terrific', 0.7745215892791748), ('phenomenal', 0.7420392632484436), ('fabulous', 0.7302290797233582), ('exciting', 0.7194732427597046), ('astounding', 0.7170230150222778), ('magnificent', 0.7014946341514587), ('fun', 0.6917933821678162), ('impressive', 0.6903508901596069), ('brilliant', 0.6854715347290039), ('beautiful', 0.6718443632125854), ('splendid', 0.6657859086990356), ('breathtaking', 0.6632450222969055)]\n",
      "\n",
      "time:\n",
      "[('when', 0.8607903122901917), ('this', 0.8540236949920654), ('before', 0.8528215885162354), ('but', 0.8492369055747986), ('only', 0.8479640483856201), ('one', 0.846072793006897), ('.', 0.8442814350128174), ('same', 0.8389487862586975), ('first', 0.8331431746482849), ('days', 0.8306636214256287), ('day', 0.827640950679779), ('so', 0.8265266418457031), ('once', 0.8218910098075867), ('even', 0.8217733502388), ('just', 0.819922149181366), ('next', 0.8190840482711792), ('again', 0.8176407217979431), ('years', 0.8145484328269958), ('way', 0.8143361210823059), ('though', 0.8142960071563721)]\n"
     ]
    }
   ],
   "source": [
    "test_words = [\"people\", \"smile\", \"amazing\", \"time\"]\n",
    "for word in test_words:\n",
    "    print(f\"\\n{word}:\")\n",
    "    print(glove.most_similar(word, topn=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1c02093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,\n",
       "       -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,\n",
       "       -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,\n",
       "       -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,\n",
       "        0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,\n",
       "        0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,\n",
       "       -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,\n",
       "        0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,\n",
       "        0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,\n",
       "       -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,\n",
       "        0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,\n",
       "       -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,\n",
       "        0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,\n",
       "       -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,\n",
       "       -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,\n",
       "        0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,\n",
       "       -0.33089 , -0.52881 ,  0.16483 , -0.98878 ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.get_vector('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e37eb",
   "metadata": {},
   "source": [
    "## Task 3 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7689d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification training input: df_train “Tweet” column;\n",
    "# training labels: df_train “Intensity Class” column;\n",
    "# test input: df_test “Tweet” column;\n",
    "# test labels: df_test “Intensity Class” column.\n",
    "# ● Use the token embeddings, which are generated by the embedding models you created\n",
    "# (experiment with different kinds of embeddings you created: word2vec – skip-gram and\n",
    "# word2vec-CBOW), as features to train logistic regression classifiers.\n",
    "# ● Use the token embeddings generated by the pretrained GloVe model as features to train\n",
    "# another logistic regression classifier.\n",
    "# ● Compare the performance of different logistic regression models created in the previous two\n",
    "# steps. Discuss your observations.\n",
    "# ● Compare the models that use embeddings in this assignment and the models created in\n",
    "# assignment 3 using count-based features. Discuss your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2dda70dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity\n",
      "neutral     586\n",
      "negative    378\n",
      "positive    217\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train = train_data[['Tweet']]\n",
    "y_train = train_data[['Intensity']]\n",
    "X_test = test_data[['Tweet']]\n",
    "y_test = test_data[['Intensity']]\n",
    "\n",
    "print(train_data['Intensity'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ec059d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, auc\n",
    "\n",
    "\n",
    "lg = LogisticRegression(random_state=0, solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22ec236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def transform_data_for_word_model(model, data_df):\n",
    "    v = model.wv.get_vector('king')\n",
    "    X = np.zeros((len(data_df), v.shape[0]))\n",
    "    n = 0\n",
    "    for index, row in data_df.iterrows():\n",
    "        tokens = row[\"Tweet\"].split()\n",
    "        vecs = []\n",
    "        m = 0\n",
    "        emptycount = 0\n",
    "        for word in tokens:\n",
    "            try:\n",
    "                # throws KeyError if word not found\n",
    "                vec = model.wv.get_vector(word)\n",
    "                vecs.append(vec)\n",
    "                m += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if len(vecs) > 0:\n",
    "            vecs = np.array(vecs)\n",
    "            X[n] = vecs.mean(axis=0)\n",
    "        else:\n",
    "            emptycount += 1\n",
    "        n+=1\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1624dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_cbow = transform_data_for_word_model(cbow_model,X_train)\n",
    "xtest_cbow = transform_data_for_word_model(cbow_model,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "306e86cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_reg(xtrain,y_train,xtest,y_test):\n",
    "    lg = LogisticRegression(random_state=0, solver='liblinear')\n",
    "    lg.fit(xtrain, y_train)\n",
    "    predictions = lg.predict(xtest)\n",
    "\n",
    "    print(\"Accuracy score: \", accuracy_score(y_test, predictions))\n",
    "    print(\"Precision score: \", precision_score(y_test, predictions, average=\"weighted\"))\n",
    "    print(\"Recall score: \", recall_score(y_test, predictions, average = \"weighted\"))\n",
    "    print(\"F1 score: \", f1_score(y_test, predictions, average = \"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3326980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.4791889007470651\n",
      "Precision score:  0.2296220025991806\n",
      "Recall score:  0.4791889007470651\n",
      "F1 score:  0.3104701535864823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/textmining/lib/python3.12/site-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/anaconda3/envs/textmining/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "log_reg(xtrain_cbow,y_train,xtest_cbow, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46b4fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_skip = transform_data_for_word_model(skipgram_model,X_train)\n",
    "xtest_skip = transform_data_for_word_model(skipgram_model,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6d1394d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.4727854855923159\n",
      "Precision score:  0.2593118343447321\n",
      "Recall score:  0.4727854855923159\n",
      "F1 score:  0.30991406707651653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/textmining/lib/python3.12/site-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "log_reg(xtrain_skip,y_train,xtest_skip, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a81c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_for_glove(model, data_df):\n",
    "    v = model.get_vector('king')\n",
    "    X = np.zeros((len(data_df), v.shape[0]))\n",
    "    n = 0\n",
    "    for index, row in data_df.iterrows():\n",
    "        tokens = row[\"Tweet\"].split()\n",
    "        vecs = []\n",
    "        m = 0\n",
    "        emptycount = 0\n",
    "        for word in tokens:\n",
    "            try:\n",
    "                # throws KeyError if word not found\n",
    "                vec = model.get_vector(word)\n",
    "                vecs.append(vec)\n",
    "                m += 1\n",
    "            except KeyError:\n",
    "                pass\n",
    "        if len(vecs) > 0:\n",
    "            vecs = np.array(vecs)\n",
    "            X[n] = vecs.mean(axis=0)\n",
    "        else:\n",
    "            emptycount += 1\n",
    "        n+=1\n",
    "    return X\n",
    "\n",
    "\n",
    "xtrain_glove = transform_data_for_glove(glove,X_train)\n",
    "xtest_glove = transform_data_for_glove(glove,X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ba219adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score:  0.5208110992529349\n",
      "Precision score:  0.5302978450289926\n",
      "Recall score:  0.5208110992529349\n",
      "F1 score:  0.5021804438200395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/textmining/lib/python3.12/site-packages/sklearn/utils/validation.py:1339: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "log_reg(xtrain_glove,y_train,xtest_glove, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150196be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textmining",
   "language": "python",
   "name": "textmining"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
